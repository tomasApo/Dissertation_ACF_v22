\section{Software Considerations}

\raggedright
In the process of software selection, multiple evaluations needed to take place; Access to licensing, the time required to learn the software to an acceptable degree, software features, flexibility, pricing. 

\subsubsection{Matlab}

Matlab is the standard company pick for a mathematical tool and to generate the graphs. Unfortunately, licensing issues and multiple versions available made this process very time-consuming. The last issue was the learning curve being very steep and thus requiring lots of hours to receive little in added benefit.

\subsubsection{Python}

Python was also in careful considering as a mathematical tool. Here the learning curve was easier and with the huge range of tutorials and documentation online, this become the program of choice. Another selling point was the importing of Libraries using `statsmodel' allowed for the integration of autocorrelation and other statistical features without having to manually code them yourself. This, the user could quickly get the desired outcome.

\subsubsection{Diadem}

The last software in consideration was Diadem, used to help engineers accelerate the post-processing of measurement data. Designed for large datasets, this would be an excellent choice. The only reason Diadem could not otherthrow Python was Python's advantages of flexibility and open-source that allowed this skill to be applied in other areas. 

\section{Libraries used in final python script}
\subsubsection{Matplotlib}
``Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible." \cite{matplotlib}

This library was originally produced by John D. Hunder American microbiologist to provide a a MATLAB-like interface. 
This library is the backbone of the authors script. All data visualisation graphs are produced using this library. For reference code refers to matplotlib as plt in script. 

\subsubsection{NumPy}

NumPy is another famous open source library created in 2006 used in a variety of fields of STEM from engineering to science and mathematics. ``It’s the universal standard for working with numerical data in Python, and it’s at the core of the scientific Python and PyData ecosystems." 

Used by beginners coders to advanced engineers in R\&D, the API is used in depth with most data science and scientific Python packages but most importantly for this tool, Matplotlib and pandas. 

The NumPy API is used extensively in Pandas, SciPy, Matplotlib, scikit-learn, scikit-image and most other data science and scientific Python packages.

For this report, NumPy is used to create NumPy arrays, mathematical function sum \cite{mhvk}, average and functions. These Numpy arrays are used instead of standard Python lists as they perform better in speed and are more compact. 

\subsubsection{Pandas}

``Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more" \cite{pandas}

Pandas in the code are referred to as the standardised name of pd. The sole purpose of this library in the report is to import and read the excel data using ``pd.read\_csv".

\subsubsection{Scripy}

Scripy, developed in 2010 by Jonas Melian. ``The main tool of this package is the shell.Run() class which lets you run system commands in the same shell. It works well with pipes and pass the shell variables." \cite{Scripy}.  

In this script it is only used to assist in down sampling the signal data. 
``signal = scipy.signal.resample(signal,123200)" 

\subsubsection{Statsmodels}

Statsmodels is a Python module that includes functions inside classes for a wide variety of statistical models, in addition to the execution of statistical tests and the study of statistical data. For each estimator, a full list of result statistics is accessible. In order to validate the accuracy of the findings, they are compared to various pre-existing statistical analysis programmes. \cite{law2019stumpy} 
Statsmodel library serves as the foundations for this technical report because it can calculate the mathematical formulas and statistics for autocorrelation in a relatively small number of lines of code.

\section{Script structure}
\begin{tikzpicture}[node distance=2cm]

\node (start) [startstop] {Start};

\node (in1) [io, below of=start] {Input data as .csv};

\node (in2) [process, below of=in1] {Resample to 100hz};

\node (pro1) [process, below of=in2] {Autocorrolate function plot};

\node (dec1) [decision, below of=pro1, yshift=-1.5cm] {More than 2 peaks?};

\node (pro2a) [process, below of=dec1, yshift=-1.5cm] {Overlay all potential cycles};
\node (pro2b) [process, right of=dec1, xshift=3.5cm] {Find new set of data};
\node (dec2) [io, below of=pro2a, yshift=-0.5cm] {Choose filtering method};

\node (min) [process, below of=dec2, xshift=-5cm] {Minimum};
\node (max) [process, below of=dec2, xshift=0cm] {Maximum};
\node (ave) [process, below of=dec2, xshift=5cm] {Average};

\node (output) [io, below of=max] {Output};
\node (stop) [startstop, below of=output] {Stop};

\draw [arrow] (start) -- (in1);
\draw [arrow] (in1) -- (in2);
\draw [arrow] (in2) -- (pro1);
\draw [arrow] (pro1) -- (dec1);
\draw [arrow] (dec1) -- node[anchor=south] {no} (pro2b);
\draw [arrow] (dec1) -- node[anchor=west] {yes} (pro2a);
\draw [arrow] (pro2a) -- (dec2);
\draw [arrow] (pro2b) |- (in1);
\draw [arrow] (dec2) -- (min);
\draw [arrow] (dec2) -- (max);
\draw [arrow] (dec2) -- (ave);

\draw [arrow] (min) -- (output);
\draw [arrow] (max) -- (output);
\draw [arrow] (ave) -- (output);

\draw [arrow] (output) -- (stop);


\end{tikzpicture}


